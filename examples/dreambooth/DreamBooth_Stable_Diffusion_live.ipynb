{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU7NuMAA2drw",
        "outputId": "cef32211-bde1-4fcc-cac9-f23ddc74ca44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4, 15109 MiB, 15109 MiB\n"
          ]
        }
      ],
      "source": [
        "#@markdown Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzM7j0ZSc_9c"
      },
      "source": [
        "https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_id = \"clcys850g000008la7xil5aas\"\n",
        "token = \"rqv\"\n",
        "gender = \"person\""
      ],
      "metadata": {
        "id": "Cb1qRYsoChgb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from distutils.dir_util import copy_tree\n",
        "copy_tree('/content/data/person', '/content/drive/MyDrive/AI_ART/sd15regularisation/person')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SywYwPPvCdH0",
        "outputId": "dec36675-759b-4352-ee8a-83b87e0d9a1d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/AI_ART/sd15regularisation/person/26-333b84135ded31b9e528adb52fca2e52b876bad4.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/154-f2fe07f7f3a7952390c74d95df2a7cd6504d08c4.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/21-231199a4ebac0b52fc8f3c6a13a6ff4cf49ebd45.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/149-3c15e3508bd5aa4d59de4c7950d4f39acf5173d7.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/102-265ef79672b9a08dfb9584ea1688407371ace2c4.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/49-1c9302b7d93579973bc012160b9ce7d0a3316d40.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/39-98a4a83d5d5aef8494dc12217b706a4ac50f0084.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/119-9573f1b8c0e72d0b7acfae2fa9ad232e273824fd.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/148-668f8c5d55cf42950d43c3ee1967dc9d0f78e106.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/189-9b8773096ad8350405191335e1a39e6202f03b0a.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/17-ffe5b143d2e697d4211818fddd6377ee2fbb19c5.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/71-dc1d46cb2b7bed250eddaf0d8c85ad6cc1e5764d.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/128-1631b6ec55d90cb6bf37e8c82e812b627c04cfd5.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/7-443d4a4bf9074616d6158fa9bb377a179b2e7967.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/35-8ca6c3ddbef1637054e589bb74756538e83df8e7.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/191-60a504edfdc6173d427be4849ecdbe269b5f8646.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/28-8241aad6c6fc2c89eba80bd9aca864a0e3604141.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/198-94d7a157f1ef558c0e3cdc60b938d2f6d766805a.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/14-9c9f0d86653f3b08112d30dfee203aa44c4e72df.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/174-fb15fd3d0ffe2392d45f0a447040e80e4b4d87ca.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/190-73b7dcc6dc8ba265c157f76b1803c3a98f54de66.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/89-c37b88be7e2b6407805bac53787c8dad7bc968bd.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/105-d6332afa326dba1523e23c81a9ce5db79c8d5190.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/171-11d12c1af05c9ae343b686d49613a843c7a9a64f.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/160-6614cc95e0f9c1e60825a9e3ca410f89e45746f6.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/109-d44dc26ce5af47013426a132ea5077770f27f081.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/65-53cde8235442a251e3c2d8287bb6b6a51fea2039.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/59-d7ffa2329b679c8338886a26016594ac2df68c26.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/161-4845eea1f8975996b75b9e105df1d7c7b1a40714.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/45-f8c223df2d1d1cdad3f3f0f13a46a79b992c1165.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/155-75f53f1ee86f12f85051846f1ad1adbf7603bd79.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/23-7143adb1acc203341a3020bfbf4ce1aebbcc94cd.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/29-22ca3fa353e1555a7783669eac478acedc57d4ab.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/175-1d4ce80a1e32625f57beddca5f27dc829a896531.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/124-a2fbb3fb67461739bd3df0005f11fd0b1f597dac.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/54-3e1942344b424b9e6bf85e903a2b0aa55dc513a3.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/80-fe40ca54990273b8a7421b1593e7c11fcf246fa5.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/127-2d7eeb7ea560aed16ead8751f02a4981e9f06129.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/99-9ada65e42c4f807c5bdd9a86c13be4d9eef947e1.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/152-c34ad23815acd71fcd3c8f10f9b26b2cb00493f3.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/156-0a9e962ac958514f28845e8fe9d9b07a80591651.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/64-6f1087745ffc94f13a479f39f0c75121807e74a2.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/3-1796b4778badd7d1c2d9817629b417045b8713cc.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/30-84e06b88ace524a60f9c4ed05ed67b3fafb04434.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/125-5a5eed59521fb22c4c6b98fdcc2ee34de237dd4c.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/13-77eae4d64e3cb457b661c8138f40eaa886a716df.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/41-666c36d48417a07249cf047f06792027600c7258.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/40-86d30923890faf3bbb19fe6055eeb4ce4215ad19.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/66-8c466242a33153d2f8387aa57cd8027fe8a20f5d.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/8-45b14b224f8972e02f1c7879635a09273bacf49d.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/120-ec6689a929ba870d6c01ecab1fcdd14508d868e3.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/151-8d6ca069f1d1f1c4f72a1d48ba3eec4da7dab235.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/92-a59723fcda822bbd4fde2c31c0b6471666f4cbb0.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/61-cffedeca2e993b150b58f8468ac84011942ce880.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/167-0e1139be305700e4a8562a409f051923166666a9.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/42-2a783f30c5f2e12fb4f354266ff9ca6a22b09691.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/103-51a08251ffa4f440dacff6b0acac5d02182c07ca.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/104-dde70e47c8eddda45a59574569554d6a87f35086.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/69-8e503318ebcf6ba66a01e510ffb8b5953250a1e7.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/84-26c7e684d1a4eb2298d65ef6f11abfc913c99f4f.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/164-e4f057db9afe4f7d0a75e1741cf63b20f27d6e1f.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/183-06f89ca6c57e1b0c1e3f6342502e835d9d1e3f73.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/110-c557810eeb63d00cf582801555aab3250e159346.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/82-2f1e047768c5ebbfb634a85ac01f17668254b30b.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/132-0c0bf24ebd946079263a111a64cc8a1a10d03ed1.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/150-5821084e42629a88373f09016b6e2ebd2735f74c.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/38-2e330ea984546d9eed503e39cb30ad017fd90168.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/0-85d34d1c8330d376b2ffdfaed7debc3e94a6fde7.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/118-7de9d448ddbf1720602c8d8c4fe1bbab03dcc062.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/169-b6e284c6ca3293ca57297df017cc439c891410a8.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/121-6145a85a6f9432863b8934f8596239ed890bcbee.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/195-732693df4eb05e2134410246afaf7cb6de860d8c.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/142-824232fec3da71b2d24a6088c9afd4d20f3971ee.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/75-3ca38f5f122045895f331b214108c163deddec0b.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/88-5933a7c54ca0f977d37c64778a386e945d6633bd.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/4-295bd6132e94d8d4c5feeb99dd743f59907cfecf.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/159-ff6edd7cc7db497e345aec0b4c15e71908b07de7.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/22-c7739fab084db5a5565d6e10770182a57abe3c4f.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/100-548129bf513e7ff4dcb2a203ed7862ee8e4ea40a.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/70-570d1f1efc8617573d7708b059e2762fbdf4847e.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/115-e0c4c48d9a157761fbff49abc9c46f403578d6d7.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/5-7fbc1728d95f7c9dadd1c88c0a7d91bd59b6dbb7.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/56-c4b5a477e1f798e2f90b12d8278ca99ea4c03ec7.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/93-4ebe658cda2b96b4d1ff9a884821e0b1f2a1ec62.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/117-a9ffdc025f8819f7d0bf4c01f34384efeee7cd6f.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/114-cffc4bbea4277d375a621cf202d2b9391f3c7ab9.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/147-bc011dfc4cec28baf9c91571bdd88ec336f5c741.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/86-adb21634da44331e2d8cc9ad351b5dd3112f766a.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/68-6c97fb5061fe1ca385e11cbf041df63db82723ef.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/137-741e882d84fbbdcaa83f9f895d8a4bfc6ef7ee6e.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/81-73bad73cc00012212c5461eabe73d4480c6744dc.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/55-8182fe8a0da6fc1744f895abddb023cc3e566802.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/186-8f6785431e2b7ea0ad449d69a6fd70df2c6997c5.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/101-558a4f95bef54fd2dde9168f8a702fb87654ba7e.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/139-6d052354a3e686bb5189a3f00780ffb23bcaf2a3.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/136-a8917f465f0c083aff6f9829888c1a52715610fc.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/126-6cf8b17aab7d965ee209391733aa6f443f72c2eb.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/16-7615a067504a5f6330261d360eafd53303511865.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/173-d0eb0dbee390d3712c8f816d3a595fb9615d1070.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/140-e3cb72ff18a2795ef7569977c595a1af7a16f65c.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/153-b25eadbef88ca7216c76e6bc566aafcc1e689e99.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/166-c5868c1bededb5f30dcfbaf6281ba42145fda05b.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/95-8cc73f98d5e84b403340008ae980e367c4310012.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/15-4dad21e9b6ad7b6b759140d52d95d4f593c1a3ac.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/36-d115ed5307d02ebc9a5a9c1413051b0b3f50b9cd.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/20-196501572a9505cbb576e12d1e0eb3e2ab1342f2.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/67-0b666956cafba83fe08623f33b5f3bb0775be263.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/181-c098ed871a3fe72192e4be120dc72e3b35b38f8b.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/96-07013bf19b07e413ae63f63270c5aab0d2010615.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/192-630673e3d0b6403dfeb54f03ac6edd71888cfbef.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/122-f7312caa002c59d12b0a4a61d3f0f2d058f84ace.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/19-cb046fa5f98bf0b96b17110b1f016fe64b404312.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/197-e0839b3e9748416434a2e295092ee8e85d3e473c.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/94-46a4de3aff5508a76d68324a2b726453bd1bfe12.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/145-909bf70e7cd538ec75affec1723a0802349aca75.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/72-afb786581bbf67e4ace1b3d5ec7dd33d0ca1a4f3.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/52-24aaece1e8ebae13dd7e07312a8f8b881a889a4e.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/182-5c051de64787892ced8dbb7359f96698b46df723.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/44-aa24adcd98d3024b4c096e2d5a4d6886b9083441.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/50-57eb36ed2aa7371da05bd30f0288615a01d810ef.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/58-6cbb391870938cf796d65eec62ffb1d4fb6c765d.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/106-c5e1d4187c21338710b67447dcaff9429b1844dd.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/107-7e5c3b7eacb315d8e74563c114eee383866b9157.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/123-031f68650873ba913177c3ebcdf7f7a517a997e0.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/78-368e9f6013961a2762e452605f216982b5ccbea1.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/141-8b31eb71fea1d4314b7d8ef1b6eb59b167c11434.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/74-cb3a3bf97de4848ecb64c1e4490682d4426ce235.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/79-37acf1c60cf07968c27e2c6f88614c10eb9077b0.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/134-44fa3dbf75f2738574593b1e376a58e99445f9f6.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/6-64eba526231287960ac62be774a8ee6deb2ac620.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/43-0598474ca07863f689968bd7fbfdf3f7405edaf6.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/27-51de18dcd2f5861d60e99418149e75fc30d14474.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/177-ad36858781d158e9f28d2e362ea7c075cb0adf27.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/116-4e26c883cc8421c0a304df10553df9f0ffbd82dd.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/2-b06e64bea6cc828a2beddb4369350fd2e76284d5.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/113-9add7fd6e5cafdb046ad9d04c1a77fa41942ec21.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/165-26a8d45b2390c197679f5e6e62dbe7f478ac302f.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/18-7b5aa04535423b2cf10c5c16d969d1624703e539.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/51-5e8ad40c6a1a88b0ff3bfb3bc4fba94d7116f14b.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/184-809fe025fd5851f512fb9826e13ba34a11b84a11.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/168-c7f4c9eb9d45ada943b8e9071245f070260af74d.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/196-d7b3fb3dccf8d5a77467cb19af14a9aee2da126e.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/85-e9a42cae2606eedec5b2484c176deb968a77f5c2.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/188-656dd9b20b333c7671ecb171ef22bb80649512c2.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/187-dae59c29004f0072ae595ec0f9eff85669d58532.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/31-a98ab03ad2b28b3647b4b5c3b92b28b1d583c293.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/185-19002ef9b5aef136cf1a29ddc3b7b92669dc0d09.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/180-98cdd5a49c0164e5afb1efb4b750903e9930903f.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/77-76575537743ae0fc0fbda632b84c2f75329ea5cb.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/144-2376862ac018de012842f61fc78dfc5ee5d9c119.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/10-01e47e6f4801342d68eead3435bfb7064454bb8f.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/163-cee2775ce070fe2f83ca2f342487c7e7ec29d262.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/98-1e74e8cea5f8dd58f4b438d41fce409b1ae0889f.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/133-d4282ee8cb22be39219cdfff4a2a494c5dafbd2d.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/11-c27c07203a0f624012ca59b63f23e0fc8f04601f.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/97-8972c773669bc9f1974d21c5a6b8700e85d71a85.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/178-ecea174eec9e5acef1d98fe32b9dc8e63d6f5a77.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/33-6f143d91346451314961b8f1b18d9bf1aabfcee1.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/83-57e1ae2bb31b4dbaf0dce4ec5263a847a1d3d864.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/91-4a3c67efb7d128d84ac23609e295954b62c116bb.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/1-3176b29bcbc90b9f31ecee88c8039f5ffbc96d3d.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/170-1974d9789a3112d60fe637932014a4b00b3894ab.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/87-11e91174439c6339a4aa0a6f9e7fd44d374a3f66.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/158-0dcbe24480d957535110025e09b15e1a208a6fd5.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/63-daf029bbe7eccd0a0bd30ccdb4c16bdeacdce0d0.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/48-745093cba1898499c9c300241789365fa0b43743.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/60-3526bc6a1fe3be941d2784ed28958a4cb343217e.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/53-bbb73cac432c7d7fb57247d8c575bf25d92fe487.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/24-98b0f0339174b2f37174022e1d4db7062ceb8952.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/179-5141effc731ca29759c78d1b6995808fda140de8.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/90-c14c5560002af8d833bce9f39b227303f72dba29.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/112-bddfe1446adfa61f66fd2553ff80d94f4c8419e4.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/111-08c0ca852c4afa441c32b1bcb4b9bde5fbbbcfed.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/25-f91c62be86f7967040c2aee1f4d611745cbb3919.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/193-feb85fe3bffb2f22e5f36cc08db341a6e3f204ea.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/146-376d425f7ac86978269629df0e87037eebdbec58.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/130-715395e1e831d13b8c5cb65df40d1ae8cf53af8f.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/37-309914a9993cb8962bc3f47b99a101cee87de387.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/172-01e4d64f69d9624a19f41761520db15077b262e5.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/131-ef5b6bb473f82414ea4ce3cbc55c937ef1f9e9ab.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/32-73b78a7134d4f25acc52ad58f08fb341f6709535.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/46-c442e3adb25b28490c99587fbdc2584fb9fc9e4a.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/135-5beb2514e49749d99ca812cc7b19bc840e2e30db.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/62-9b3c17bb085e38312981bc30e424ca9720f23fa7.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/12-9c1c0f22aef5631582a37e2f5b0a577c59d6d0b6.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/34-07c183e6830f483b33ea5f140cb47c9c0136e641.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/73-36d17466cabae18ead8225e511012bf6ae854ebc.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/47-24dfbf36f2f4db6f975515d4f03e4f4f2ca5b484.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/138-01e85c3ec17a6bb1bf55a1ddfa69cf41891467e6.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/157-4a6423c4296bed1242cecb13acab4c05361e19b3.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/194-a0095d250f798b142d68f25be5bf22f1136fcb27.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/176-c66dd52dfb1e02a60949214e1023f2478d09214b.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/57-2ad604c34e956b5e584487f3018083f32a974e70.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/108-28ad0b9f876369fcffb7c58b56c4c0d2b45164ac.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/9-930c43e7f0b66219eab161ec45cc2d57979c5c63.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/143-5676a8ad3575a5bfe37f8d710020f6adb708b2ba.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/129-e0a7663231bd2834c4b76816bf7300836924b665.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/162-f9cc3941ef417a08415ef803271351cb12ad85e6.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/199-32b8ce242e78cfc4c40f715f54fe0a0cf7a93a5c.jpg',\n",
              " '/content/drive/MyDrive/AI_ART/sd15regularisation/person/76-488818df998403bc8728cd610821842ed2a78102.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnTMyW41cC1E"
      },
      "source": [
        "## Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aLWXPZqjsZVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b9c328b-6cb2-44b4-dbf5-449814fab95d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://github.com/KidCon/diffusers_ss/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y4lqqWT_uxD2"
      },
      "outputs": [],
      "source": [
        "#@title Login to HuggingFace 🤗\n",
        "\n",
        "#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree. You have to be a registered user in 🤗 Hugging Face Hub, and you'll also need to use an access token for the code to work.\n",
        "# https://huggingface.co/settings/tokens\n",
        "!mkdir -p ~/.huggingface\n",
        "HUGGINGFACE_TOKEN = \"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\" #@param {type:\"string\"}\n",
        "# HUGGINGFACE_TOKEN = \"\" #@param {type:\"string\"}\n",
        "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfTlc8Mqb8iH"
      },
      "source": [
        "### Install xformers from precompiled wheel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6dcjPnnaiCn",
        "outputId": "f78d79df-bf83-4754-c780-3812dec02198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --no-deps -q https://github.com/brian6091/xformers-wheels/releases/download/0.0.15.dev0%2B4c06c79/xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl\n",
        "# These were compiled on Tesla T4.\n",
        "\n",
        "# If precompiled wheels don't work, install it with the following command. It will take around 40 minutes to compile.\n",
        "# %pip install git+https://github.com/facebookresearch/xformers@4c06c79#egg=xformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NV324ZcL9L"
      },
      "source": [
        "## Settings and run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Rxg0y5MBudmd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd757ea-82d2-4f4d-f894-5966e6bcfe61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[*] Weights will be saved at /content/drive/MyDrive/stable_diffusion_weights/live_subjects/clcys850g000008la7xil5aas\n"
          ]
        }
      ],
      "source": [
        "#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "save_to_gdrive = True #@param {type:\"boolean\"}\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Name/Path of the initial model.\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = f\"stable_diffusion_weights/live_subjects/{char_id}\"\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "# Start Training\n",
        "\n",
        "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ioxxvHoicPs"
      },
      "source": [
        "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
        "\n",
        "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
        "\n",
        "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if gender == \"person\":\n",
        "  class_data_dir = '/content/drive/MyDrive/AI_ART/sd15regularisation/person'\n",
        "elif gender == \"man\":\n",
        "  class_data_dir = '/content/drive/MyDrive/AI_ART/sd15regularisation/man'\n",
        "elif gender == \"woman\":\n",
        "  class_data_dir = '/content/drive/MyDrive/AI_ART/sd15regularisation/woman'\n",
        "  \n"
      ],
      "metadata": {
        "id": "mJ0mJ2mzEw8r"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5vDpCxId1aCm"
      },
      "outputs": [],
      "source": [
        "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
        "\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      f\"photo of {token} {gender}\",\n",
        "        \"class_prompt\":         f\"photo of a {gender}\",\n",
        "        \"instance_data_dir\":    f\"/content/data/{token}\",\n",
        "        \"class_data_dir\":       class_data_dir\n",
        "    },\n",
        "#     {\n",
        "#         \"instance_prompt\":      \"photo of ukj person\",\n",
        "#         \"class_prompt\":         \"photo of a person\",\n",
        "#         \"instance_data_dir\":    \"/content/data/ukj\",\n",
        "#         \"class_data_dir\":       \"/content/data/person\"\n",
        "#     }\n",
        "]\n",
        "\n",
        "# `class_data_dir` contains regularization images\n",
        "import json\n",
        "import os\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "32gYIDDR1aCp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "7153c954-5a8d-40b6-8940-ae062a7903d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading instance images for `photo of rqv person`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-45de40ab-909b-4341-a5da-7790bde30af2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-45de40ab-909b-4341-a5da-7790bde30af2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown Upload your images by running this cell.\n",
        "\n",
        "#@markdown OR\n",
        "\n",
        "#@markdown You can use the file manager on the left panel to upload (drag and drop) to each `instance_data_dir` (it uploads faster)\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "for c in concepts_list:\n",
        "    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
        "        shutil.move(filename, dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7205a6-1b82-4d5e-e70a-898f17c19db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:accelerate.commands.launch:The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrdp\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230117_093958-tlira4ju\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtreasured-dew-10\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rdp/dreambooth-live-subjects\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rdp/dreambooth-live-subjects/runs/tlira4ju\u001b[0m\n",
            "adding class_images to artifact\n",
            "artifact: <wandb.sdk.wandb_artifacts.Artifact object at 0x7f054dfd9e80>, concept:{'instance_prompt': 'photo of rqv person', 'class_prompt': 'photo of a person', 'instance_data_dir': '/content/data/rqv', 'class_data_dir': '/content/drive/MyDrive/AI_ART/sd15regularisation/person'}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/drive/MyDrive/AI_ART/sd15regularisation/person)... Done. 0.5s\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
            "================================================================================\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:105: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--listen_host=172.28.0.2 --target_host=172.28.0.2 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-32xgkzozxpzw9 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('{\"kernelManagerProxyPort\"'), PosixPath('[\"--ip=172.28.0.2\"],\"debugAdapterMultiplexerPath\"'), PosixPath('\"172.28.0.2\",\"jupyterArgs\"'), PosixPath('\"/usr/local/bin/dap_multiplexer\",\"enableLsp\"'), PosixPath('6000,\"kernelManagerProxyHost\"'), PosixPath('true}')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 112\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda112.so...\n",
            "/usr/local/lib/python3.8/dist-packages/diffusers/utils/deprecation_utils.py:35: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
            "  warnings.warn(warning + message, FutureWarning)\n",
            "Caching latents: 100% 200/200 [00:43<00:00,  4.60it/s]\n",
            "***** Running training *****\n",
            "  Num examples = 200\n",
            "  Num batches each epoch = 200\n",
            "  Num Epochs = 40\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 8000\n",
            "Steps:  10% 825/8000 [11:58<1:43:09,  1.16it/s, loss=0.291, lr=1e-6]"
          ]
        }
      ],
      "source": [
        "!accelerate launch train_dreambooth.py \\\n",
        "  --special_save_weights=10 \\\n",
        "  --wandb_project=\"dreambooth-live-subjects\" \\\n",
        "  --prior_preservation_images=\"generated\" \\\n",
        "  --minimum_save_step=1999 \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=200 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=8000 \\\n",
        "  --save_interval=1000 \\\n",
        "  --save_sample_prompt=f\"photo of {token} {gender}\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89Az5NUxOWdy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "29eaf3b0-5fdb-45ab-c8cd-72d409da6735"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1046691759c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mWEIGHTS_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "22#@markdown Specify the weights directory to use (leave blank for latest)\n",
        "WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACUL_oKlLwBt"
      },
      "outputs": [],
      "source": [
        "#@markdown Run to generate a grid of preview images from the last saved weights.\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "weights_folder = OUTPUT_DIR\n",
        "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
        "\n",
        "row = len(folders)\n",
        "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
        "scale = 4\n",
        "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "    folder_path = os.path.join(weights_folder, folder)\n",
        "    image_folder = os.path.join(folder_path, \"samples\")\n",
        "    images = [f for f in os.listdir(image_folder)]\n",
        "    for j, image in enumerate(images):\n",
        "        if row == 1:\n",
        "            currAxes = axes[j]\n",
        "        else:\n",
        "            currAxes = axes[i, j]\n",
        "        if i == 0:\n",
        "            currAxes.set_title(f\"Image {j}\")\n",
        "        if j == 0:\n",
        "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        img = mpimg.imread(image_path)\n",
        "        currAxes.imshow(img, cmap='gray')\n",
        "        currAxes.axis('off')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.savefig('grid.png', dpi=72)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V8wgU0HN-Kq"
      },
      "source": [
        "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXzsUyG1aCy"
      },
      "outputs": [],
      "source": [
        "#@markdown Run conversion.\n",
        "ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
        "\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNG4fd_dTbF"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW15FjffdTID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "\n",
        "g_cuda = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIzkltjpVO_f"
      },
      "outputs": [],
      "source": [
        "#@markdown Can set random seed here for reproducibility.\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6xoHWSsbcS3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#@title Run for generating images.\n",
        "\n",
        "prompt = \"photo of spoon\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "num_samples = 4 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "num_inference_steps = 50 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WMCqQ5Tcdsm2"
      },
      "outputs": [],
      "source": [
        "#@markdown Run Gradio UI for generating images.\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        return pipe(\n",
        "                prompt, height=int(height), width=int(width),\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_images_per_prompt=int(num_samples),\n",
        "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "                generator=g_cuda\n",
        "            ).images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=50)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lJoOgLQHnC8L"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Delete diffuser and old weights and only keep the ckpt to free up drive space.\n",
        "\n",
        "#@markdown [ ! ] Caution, Only execute if you are sure u want to delete the diffuser format weights and only use the ckpt.\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "for f in glob(OUTPUT_DIR+os.sep+\"*\"):\n",
        "    if f != WEIGHTS_DIR:\n",
        "        shutil.rmtree(f)\n",
        "        print(\"Deleted\", f)\n",
        "for f in glob(WEIGHTS_DIR+\"/*\"):\n",
        "    if not f.endswith(\".ckpt\") or not f.endswith(\".json\"):\n",
        "        try:\n",
        "            shutil.rmtree(f)\n",
        "        except NotADirectoryError:\n",
        "            continue\n",
        "        print(\"Deleted\", f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXgi8HM4c-DA"
      },
      "outputs": [],
      "source": [
        "#@title Free runtime memory\n",
        "exit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDavPmgUp5Fn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
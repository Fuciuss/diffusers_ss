{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU7NuMAA2drw",
        "outputId": "0346b34c-9ac4-4835-fb68-ed29eba026ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4, 15109 MiB, 15109 MiB\n"
          ]
        }
      ],
      "source": [
        "#@markdown Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzM7j0ZSc_9c"
      },
      "source": [
        "https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_id = \"testcallum2.1\"\n",
        "token = \"vzv\"\n",
        "gender = \"man\""
      ],
      "metadata": {
        "id": "Cb1qRYsoChgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnTMyW41cC1E"
      },
      "source": [
        "## Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aLWXPZqjsZVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe89e39-92ec-4393-eba1-0708e1c65856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m453.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m191.5/191.5 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.8/55.8 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m107.0/107.0 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m177.4/177.4 KB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.3/64.3 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://github.com/KidCon/diffusers_ss/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "y4lqqWT_uxD2"
      },
      "outputs": [],
      "source": [
        "#@title Login to HuggingFace ü§ó\n",
        "\n",
        "#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree. You have to be a registered user in ü§ó Hugging Face Hub, and you'll also need to use an access token for the code to work.\n",
        "# https://huggingface.co/settings/tokens\n",
        "!mkdir -p ~/.huggingface\n",
        "HUGGINGFACE_TOKEN = \"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\" #@param {type:\"string\"}\n",
        "# HUGGINGFACE_TOKEN = \"\" #@param {type:\"string\"}\n",
        "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfTlc8Mqb8iH"
      },
      "source": [
        "### Install xformers from precompiled wheel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6dcjPnnaiCn",
        "outputId": "335ef55f-c843-404c-86d6-b8cff3885b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --no-deps -q https://github.com/brian6091/xformers-wheels/releases/download/0.0.15.dev0%2B4c06c79/xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl\n",
        "# These were compiled on Tesla T4.\n",
        "\n",
        "# If precompiled wheels don't work, install it with the following command. It will take around 40 minutes to compile.\n",
        "# %pip install git+https://github.com/facebookresearch/xformers@4c06c79#egg=xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall xformers"
      ],
      "metadata": {
        "id": "YqI6tQ40Hxz7",
        "outputId": "86032b1a-bdfc-4af9-91a3-82ce58cd56b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: xformers 0.0.15.dev0+4c06c79.d20221205\n",
            "Uninstalling xformers-0.0.15.dev0+4c06c79.d20221205:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.8/dist-packages/experimental/*\n",
            "    /usr/local/lib/python3.8/dist-packages/xformers-0.0.15.dev0+4c06c79.d20221205.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/xformers/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled xformers-0.0.15.dev0+4c06c79.d20221205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xformers"
      ],
      "metadata": {
        "id": "3ZLiR1IgIBXF",
        "outputId": "af49c2ec-2c11-486a-c732-1f8800571566",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.13-cp38-cp38-manylinux_2_17_x86_64.whl (92.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyre-extensions==0.0.23\n",
            "  Downloading pyre_extensions-0.0.23-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from xformers) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.8/dist-packages (from xformers) (1.13.1+cu116)\n",
            "Collecting typing-inspect\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from pyre-extensions==0.0.23->xformers) (4.4.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, xformers\n",
            "Successfully installed mypy-extensions-0.4.3 pyre-extensions-0.0.23 typing-inspect-0.8.0 xformers-0.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NV324ZcL9L"
      },
      "source": [
        "## Settings and run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rxg0y5MBudmd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1709dec7-953e-4230-d810-044652008047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[*] Weights will be saved at /content/drive/MyDrive/stable_diffusion_weights/live_subjects/clcyv4i9d000108kxbn41b79k\n"
          ]
        }
      ],
      "source": [
        "#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "save_to_gdrive = True #@param {type:\"boolean\"}\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Name/Path of the initial model.\n",
        "MODEL_NAME = \"stabilityai/stable-diffusion-2-1\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = f\"stable_diffusion_weights/live_subjects/{char_id}\"\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "# Start Training\n",
        "\n",
        "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ioxxvHoicPs"
      },
      "source": [
        "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
        "\n",
        "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
        "\n",
        "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if gender == \"person\":\n",
        "  class_data_dir = '/content/drive/MyDrive/AI_ART/sd21regularisation/person'\n",
        "elif gender == \"man\":\n",
        "  class_data_dir = '/content/drive/MyDrive/AI_ART/sd21regularisation/man'\n",
        "elif gender == \"woman\":\n",
        "  class_data_dir = '/content/drive/MyDrive/AI_ART/sd21regularisation/woman'\n",
        "  \n"
      ],
      "metadata": {
        "id": "mJ0mJ2mzEw8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vDpCxId1aCm"
      },
      "outputs": [],
      "source": [
        "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
        "\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      f\"photo of {token} {gender}\",\n",
        "        \"class_prompt\":         f\"photo of a {gender}\",\n",
        "        \"instance_data_dir\":    f\"/content/data/{token}\",\n",
        "        \"class_data_dir\":       class_data_dir\n",
        "    },\n",
        "#     {\n",
        "#         \"instance_prompt\":      \"photo of ukj person\",\n",
        "#         \"class_prompt\":         \"photo of a person\",\n",
        "#         \"instance_data_dir\":    \"/content/data/ukj\",\n",
        "#         \"class_data_dir\":       \"/content/data/person\"\n",
        "#     }\n",
        "]\n",
        "\n",
        "# `class_data_dir` contains regularization images\n",
        "import json\n",
        "import os\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32gYIDDR1aCp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "88564cd1-7af1-48f8-bd70-aab327e03b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading instance images for `photo of vzv woman`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2d6fc6f3-5911-4d4d-8230-f8f0b6e8b5d0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2d6fc6f3-5911-4d4d-8230-f8f0b6e8b5d0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown Upload your images by running this cell.\n",
        "\n",
        "#@markdown OR\n",
        "\n",
        "#@markdown You can use the file manager on the left panel to upload (drag and drop) to each `instance_data_dir` (it uploads faster)\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "for c in concepts_list:\n",
        "    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
        "        shutil.move(filename, dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_instance_images = len(os.listdir(concepts_list[0][\"instance_data_dir\"]))\n",
        "special_weights_coeff = 120\n",
        "save_special_weights = num_instance_images * special_weights_coeff\n",
        "print(f'saving special weights at: {save_special_weights}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBeM7WjBLeG5",
        "outputId": "a1f18d7c-aef1-4f89-a54f-098275c3b118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving special weights at: 2640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(save_special_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjgqnXLaMeVt",
        "outputId": "fafb9f5d-7d88-40e3-dd3d-d56e2fd20aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc374e6-d83c-42e0-a869-cc0f4d2f735b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:accelerate.commands.launch:The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230117_100724-bky1861i\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33molive-puddle-11\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/rdp/dreambooth-live-subjects\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/rdp/dreambooth-live-subjects/runs/bky1861i\u001b[0m\n",
            "adding class_images to artifact\n",
            "artifact: <wandb.sdk.wandb_artifacts.Artifact object at 0x7f0314e4bd00>, concept:{'instance_prompt': 'photo of vzv woman', 'class_prompt': 'photo of a woman', 'instance_data_dir': '/content/data/vzv', 'class_data_dir': '/content/drive/MyDrive/AI_ART/sd15regularisation/woman'}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/drive/MyDrive/AI_ART/sd15regularisation/woman)... Done. 0.0s\n",
            "Downloading: 100% 335M/335M [00:03<00:00, 87.8MB/s]\n",
            "Downloading: 100% 547/547 [00:00<00:00, 619kB/s]\n",
            "Downloading: 100% 543/543 [00:00<00:00, 587kB/s]\n",
            "Fetching 15 files:   0% 0/15 [00:00<?, ?it/s]\n",
            "Downloading: 100% 342/342 [00:00<00:00, 365kB/s]\n",
            "\n",
            "Downloading: 100% 4.70k/4.70k [00:00<00:00, 4.85MB/s]\n",
            "Fetching 15 files:  20% 3/15 [00:00<00:00, 27.00it/s]\n",
            "Downloading:   0% 0.00/608M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading:   1% 7.57M/608M [00:00<00:07, 75.7MB/s]\u001b[A\n",
            "Downloading:   3% 16.1M/608M [00:00<00:07, 81.2MB/s]\u001b[A\n",
            "Downloading:   4% 24.3M/608M [00:00<00:07, 81.4MB/s]\u001b[A\n",
            "Downloading:   6% 33.5M/608M [00:00<00:06, 86.0MB/s]\u001b[A\n",
            "Downloading:   7% 43.3M/608M [00:00<00:06, 90.0MB/s]\u001b[A\n",
            "Downloading:   9% 52.3M/608M [00:00<00:06, 90.1MB/s]\u001b[A\n",
            "Downloading:  10% 61.9M/608M [00:00<00:05, 92.0MB/s]\u001b[A\n",
            "Downloading:  12% 71.7M/608M [00:00<00:05, 94.0MB/s]\u001b[A\n",
            "Downloading:  13% 81.1M/608M [00:00<00:06, 87.6MB/s]\u001b[A\n",
            "Downloading:  15% 89.9M/608M [00:01<00:06, 83.3MB/s]\u001b[A\n",
            "Downloading:  16% 99.1M/608M [00:01<00:05, 85.7MB/s]\u001b[A\n",
            "Downloading:  18% 109M/608M [00:01<00:05, 88.2MB/s] \u001b[A\n",
            "Downloading:  19% 117M/608M [00:01<00:05, 82.0MB/s]\u001b[A\n",
            "Downloading:  21% 127M/608M [00:01<00:05, 85.0MB/s]\u001b[A\n",
            "Downloading:  22% 136M/608M [00:01<00:05, 87.1MB/s]\u001b[A\n",
            "Downloading:  24% 146M/608M [00:01<00:05, 89.7MB/s]\u001b[A\n",
            "Downloading:  25% 155M/608M [00:01<00:05, 82.6MB/s]\u001b[A\n",
            "Downloading:  27% 164M/608M [00:01<00:05, 86.5MB/s]\u001b[A\n",
            "Downloading:  28% 173M/608M [00:01<00:05, 86.8MB/s]\u001b[A\n",
            "Downloading:  30% 182M/608M [00:02<00:04, 87.2MB/s]\u001b[A\n",
            "Downloading:  31% 191M/608M [00:02<00:04, 89.8MB/s]\u001b[A\n",
            "Downloading:  33% 202M/608M [00:02<00:04, 93.6MB/s]\u001b[A\n",
            "Downloading:  35% 211M/608M [00:02<00:04, 92.4MB/s]\u001b[A\n",
            "Downloading:  36% 221M/608M [00:02<00:04, 93.1MB/s]\u001b[A\n",
            "Downloading:  38% 230M/608M [00:02<00:04, 94.0MB/s]\u001b[A\n",
            "Downloading:  39% 240M/608M [00:02<00:03, 94.7MB/s]\u001b[A\n",
            "Downloading:  41% 249M/608M [00:02<00:03, 93.6MB/s]\u001b[A\n",
            "Downloading:  43% 259M/608M [00:02<00:04, 85.5MB/s]\u001b[A\n",
            "Downloading:  44% 268M/608M [00:03<00:03, 87.1MB/s]\u001b[A\n",
            "Downloading:  46% 277M/608M [00:03<00:03, 87.7MB/s]\u001b[A\n",
            "Downloading:  47% 286M/608M [00:03<00:03, 90.3MB/s]\u001b[A\n",
            "Downloading:  49% 296M/608M [00:03<00:03, 84.9MB/s]\u001b[A\n",
            "Downloading:  50% 305M/608M [00:03<00:03, 86.7MB/s]\u001b[A\n",
            "Downloading:  52% 314M/608M [00:03<00:03, 89.4MB/s]\u001b[A\n",
            "Downloading:  53% 323M/608M [00:03<00:03, 89.8MB/s]\u001b[A\n",
            "Downloading:  55% 332M/608M [00:03<00:03, 87.5MB/s]\u001b[A\n",
            "Downloading:  56% 342M/608M [00:03<00:02, 89.1MB/s]\u001b[A\n",
            "Downloading:  58% 352M/608M [00:03<00:02, 92.0MB/s]\u001b[A\n",
            "Downloading:  59% 361M/608M [00:04<00:02, 88.0MB/s]\u001b[A\n",
            "Downloading:  61% 371M/608M [00:04<00:02, 90.8MB/s]\u001b[A\n",
            "Downloading:  63% 380M/608M [00:04<00:02, 92.3MB/s]\u001b[A\n",
            "Downloading:  64% 390M/608M [00:04<00:02, 91.5MB/s]\u001b[A\n",
            "Downloading:  66% 399M/608M [00:04<00:02, 87.7MB/s]\u001b[A\n",
            "Downloading:  67% 408M/608M [00:04<00:02, 86.6MB/s]\u001b[A\n",
            "Downloading:  69% 418M/608M [00:04<00:02, 90.5MB/s]\u001b[A\n",
            "Downloading:  70% 427M/608M [00:04<00:02, 87.4MB/s]\u001b[A\n",
            "Downloading:  72% 436M/608M [00:04<00:01, 89.6MB/s]\u001b[A\n",
            "Downloading:  73% 446M/608M [00:05<00:01, 91.1MB/s]\u001b[A\n",
            "Downloading:  75% 455M/608M [00:05<00:01, 91.1MB/s]\u001b[A\n",
            "Downloading:  76% 464M/608M [00:05<00:01, 85.5MB/s]\u001b[A\n",
            "Downloading:  78% 474M/608M [00:05<00:01, 88.5MB/s]\u001b[A\n",
            "Downloading:  79% 482M/608M [00:05<00:01, 86.7MB/s]\u001b[A\n",
            "Downloading:  81% 491M/608M [00:05<00:01, 87.5MB/s]\u001b[A\n",
            "Downloading:  82% 501M/608M [00:05<00:01, 90.5MB/s]\u001b[A\n",
            "Downloading:  84% 511M/608M [00:05<00:01, 91.4MB/s]\u001b[A\n",
            "Downloading:  86% 520M/608M [00:05<00:00, 92.5MB/s]\u001b[A\n",
            "Downloading:  87% 529M/608M [00:06<00:00, 80.9MB/s]\u001b[A\n",
            "Downloading:  89% 539M/608M [00:06<00:00, 84.7MB/s]\u001b[A\n",
            "Downloading:  90% 548M/608M [00:06<00:00, 85.6MB/s]\u001b[A\n",
            "Downloading:  92% 557M/608M [00:06<00:00, 87.4MB/s]\u001b[A\n",
            "Downloading:  93% 566M/608M [00:06<00:00, 80.8MB/s]\u001b[A\n",
            "Downloading:  95% 575M/608M [00:06<00:00, 84.3MB/s]\u001b[A\n",
            "Downloading:  96% 584M/608M [00:06<00:00, 86.4MB/s]\u001b[A\n",
            "Downloading:  98% 593M/608M [00:06<00:00, 87.3MB/s]\u001b[A\n",
            "Downloading: 100% 608M/608M [00:06<00:00, 88.4MB/s]\n",
            "\n",
            "Downloading: 100% 307/307 [00:00<00:00, 330kB/s]\n",
            "\n",
            "Downloading: 100% 636/636 [00:00<00:00, 657kB/s]\n",
            "Fetching 15 files:  40% 6/15 [00:07<00:12,  1.40s/it]\n",
            "Downloading:   0% 0.00/246M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading:   3% 7.50M/246M [00:00<00:03, 75.0MB/s]\u001b[A\n",
            "Downloading:   6% 15.9M/246M [00:00<00:02, 80.1MB/s]\u001b[A\n",
            "Downloading:  10% 23.9M/246M [00:00<00:02, 75.4MB/s]\u001b[A\n",
            "Downloading:  13% 31.8M/246M [00:00<00:02, 76.6MB/s]\u001b[A\n",
            "Downloading:  17% 41.3M/246M [00:00<00:02, 83.2MB/s]\u001b[A\n",
            "Downloading:  20% 50.3M/246M [00:00<00:02, 85.6MB/s]\u001b[A\n",
            "Downloading:  24% 59.8M/246M [00:00<00:02, 88.5MB/s]\u001b[A\n",
            "Downloading:  28% 69.5M/246M [00:00<00:01, 91.2MB/s]\u001b[A\n",
            "Downloading:  32% 78.6M/246M [00:00<00:01, 87.7MB/s]\u001b[A\n",
            "Downloading:  36% 88.3M/246M [00:01<00:01, 90.6MB/s]\u001b[A\n",
            "Downloading:  40% 97.4M/246M [00:01<00:01, 90.4MB/s]\u001b[A\n",
            "Downloading:  43% 106M/246M [00:01<00:01, 90.4MB/s] \u001b[A\n",
            "Downloading:  47% 116M/246M [00:01<00:01, 81.6MB/s]\u001b[A\n",
            "Downloading:  50% 124M/246M [00:01<00:01, 83.1MB/s]\u001b[A\n",
            "Downloading:  54% 133M/246M [00:01<00:01, 84.5MB/s]\u001b[A\n",
            "Downloading:  58% 142M/246M [00:01<00:01, 84.6MB/s]\u001b[A\n",
            "Downloading:  61% 150M/246M [00:01<00:01, 85.5MB/s]\u001b[A\n",
            "Downloading:  65% 160M/246M [00:01<00:00, 87.9MB/s]\u001b[A\n",
            "Downloading:  69% 169M/246M [00:01<00:00, 89.3MB/s]\u001b[A\n",
            "Downloading:  72% 178M/246M [00:02<00:00, 84.0MB/s]\u001b[A\n",
            "Downloading:  76% 187M/246M [00:02<00:00, 86.0MB/s]\u001b[A\n",
            "Downloading:  80% 197M/246M [00:02<00:00, 89.8MB/s]\u001b[A\n",
            "Downloading:  84% 206M/246M [00:02<00:00, 89.4MB/s]\u001b[A\n",
            "Downloading:  87% 215M/246M [00:02<00:00, 84.5MB/s]\u001b[A\n",
            "Downloading:  91% 224M/246M [00:02<00:00, 87.0MB/s]\u001b[A\n",
            "Downloading:  95% 233M/246M [00:02<00:00, 87.6MB/s]\u001b[A\n",
            "Downloading: 100% 246M/246M [00:02<00:00, 86.6MB/s]\n",
            "\n",
            "Downloading: 100% 525k/525k [00:00<00:00, 44.4MB/s]\n",
            "Fetching 15 files:  53% 8/15 [00:10<00:09,  1.43s/it]\n",
            "Downloading: 100% 472/472 [00:00<00:00, 520kB/s]\n",
            "\n",
            "Downloading: 100% 822/822 [00:00<00:00, 949kB/s]\n",
            "Fetching 15 files:  67% 10/15 [00:10<00:04,  1.04it/s]\n",
            "Downloading: 100% 1.06M/1.06M [00:00<00:00, 27.3MB/s]\n",
            "\n",
            "Downloading: 100% 806/806 [00:00<00:00, 920kB/s]\n",
            "Fetching 15 files:  80% 12/15 [00:10<00:02,  1.48it/s]\n",
            "Downloading:   0% 0.00/1.72G [00:00<?, ?B/s]\u001b[A\n",
            "Downloading:   1% 9.10M/1.72G [00:00<00:18, 91.0MB/s]\u001b[A\n",
            "Downloading:   1% 18.2M/1.72G [00:00<00:18, 90.5MB/s]\u001b[A\n",
            "Downloading:   2% 27.3M/1.72G [00:00<00:19, 88.6MB/s]\u001b[A\n",
            "Downloading:   2% 36.8M/1.72G [00:00<00:18, 91.4MB/s]\u001b[A\n",
            "Downloading:   3% 46.0M/1.72G [00:00<00:19, 86.7MB/s]\u001b[A\n",
            "Downloading:   3% 54.7M/1.72G [00:00<00:20, 82.1MB/s]\u001b[A\n",
            "Downloading:   4% 64.1M/1.72G [00:00<00:19, 85.9MB/s]\u001b[A\n",
            "Downloading:   4% 73.3M/1.72G [00:00<00:18, 87.6MB/s]\u001b[A\n",
            "Downloading:   5% 82.4M/1.72G [00:00<00:18, 88.7MB/s]\u001b[A\n",
            "Downloading:   5% 91.9M/1.72G [00:01<00:17, 90.6MB/s]\u001b[A\n",
            "Downloading:   6% 101M/1.72G [00:01<00:17, 91.6MB/s] \u001b[A\n",
            "Downloading:   6% 111M/1.72G [00:01<00:17, 93.1MB/s]\u001b[A\n",
            "Downloading:   7% 120M/1.72G [00:01<00:19, 80.1MB/s]\u001b[A\n",
            "Downloading:   8% 130M/1.72G [00:01<00:18, 84.7MB/s]\u001b[A\n",
            "Downloading:   8% 139M/1.72G [00:01<00:18, 85.5MB/s]\u001b[A\n",
            "Downloading:   9% 148M/1.72G [00:01<00:18, 83.4MB/s]\u001b[A\n",
            "Downloading:   9% 157M/1.72G [00:01<00:18, 85.2MB/s]\u001b[A\n",
            "Downloading:  10% 166M/1.72G [00:01<00:17, 86.6MB/s]\u001b[A\n",
            "Downloading:  10% 175M/1.72G [00:02<00:17, 89.0MB/s]\u001b[A\n",
            "Downloading:  11% 184M/1.72G [00:02<00:18, 80.8MB/s]\u001b[A\n",
            "Downloading:  11% 193M/1.72G [00:02<00:18, 83.6MB/s]\u001b[A\n",
            "Downloading:  12% 202M/1.72G [00:02<00:18, 83.9MB/s]\u001b[A\n",
            "Downloading:  12% 210M/1.72G [00:02<00:17, 84.9MB/s]\u001b[A\n",
            "Downloading:  13% 220M/1.72G [00:02<00:17, 87.7MB/s]\u001b[A\n",
            "Downloading:  13% 230M/1.72G [00:02<00:16, 90.3MB/s]\u001b[A\n",
            "Downloading:  14% 239M/1.72G [00:02<00:16, 90.7MB/s]\u001b[A\n",
            "Downloading:  14% 248M/1.72G [00:02<00:16, 89.6MB/s]\u001b[A\n",
            "Downloading:  15% 257M/1.72G [00:02<00:16, 89.3MB/s]\u001b[A\n",
            "Downloading:  16% 267M/1.72G [00:03<00:15, 91.9MB/s]\u001b[A\n",
            "Downloading:  16% 276M/1.72G [00:03<00:15, 92.3MB/s]\u001b[A\n",
            "Downloading:  17% 285M/1.72G [00:03<00:15, 92.4MB/s]\u001b[A\n",
            "Downloading:  17% 295M/1.72G [00:03<00:18, 77.0MB/s]\u001b[A\n",
            "Downloading:  18% 304M/1.72G [00:03<00:17, 81.0MB/s]\u001b[A\n",
            "Downloading:  18% 313M/1.72G [00:03<00:16, 84.9MB/s]\u001b[A\n",
            "Downloading:  19% 322M/1.72G [00:03<00:16, 86.7MB/s]\u001b[A\n",
            "Downloading:  19% 332M/1.72G [00:03<00:15, 89.7MB/s]\u001b[A\n",
            "Downloading:  20% 341M/1.72G [00:03<00:15, 90.0MB/s]\u001b[A\n",
            "Downloading:  20% 351M/1.72G [00:04<00:14, 92.5MB/s]\u001b[A\n",
            "Downloading:  21% 360M/1.72G [00:04<00:15, 89.5MB/s]\u001b[A\n",
            "Downloading:  22% 370M/1.72G [00:04<00:14, 92.6MB/s]\u001b[A\n",
            "Downloading:  22% 380M/1.72G [00:04<00:14, 91.8MB/s]\u001b[A\n",
            "Downloading:  23% 389M/1.72G [00:04<00:14, 90.5MB/s]\u001b[A\n",
            "Downloading:  23% 398M/1.72G [00:04<00:15, 85.8MB/s]\u001b[A\n",
            "Downloading:  24% 407M/1.72G [00:04<00:15, 86.8MB/s]\u001b[A\n",
            "Downloading:  24% 416M/1.72G [00:04<00:14, 87.6MB/s]\u001b[A\n",
            "Downloading:  25% 425M/1.72G [00:04<00:15, 84.0MB/s]\u001b[A\n",
            "Downloading:  25% 434M/1.72G [00:04<00:15, 85.6MB/s]\u001b[A\n",
            "Downloading:  26% 443M/1.72G [00:05<00:14, 88.8MB/s]\u001b[A\n",
            "Downloading:  26% 452M/1.72G [00:05<00:14, 88.7MB/s]\u001b[A\n",
            "Downloading:  27% 461M/1.72G [00:05<00:14, 86.9MB/s]\u001b[A\n",
            "Downloading:  27% 471M/1.72G [00:05<00:14, 89.0MB/s]\u001b[A\n",
            "Downloading:  28% 480M/1.72G [00:05<00:14, 86.0MB/s]\u001b[A\n",
            "Downloading:  28% 489M/1.72G [00:05<00:13, 88.6MB/s]\u001b[A\n",
            "Downloading:  29% 498M/1.72G [00:05<00:14, 84.5MB/s]\u001b[A\n",
            "Downloading:  30% 508M/1.72G [00:05<00:13, 87.5MB/s]\u001b[A\n",
            "Downloading:  30% 517M/1.72G [00:05<00:13, 88.5MB/s]\u001b[A\n",
            "Downloading:  31% 526M/1.72G [00:06<00:14, 85.1MB/s]\u001b[A\n",
            "Downloading:  31% 535M/1.72G [00:06<00:13, 88.9MB/s]\u001b[A\n",
            "Downloading:  32% 545M/1.72G [00:06<00:12, 91.6MB/s]\u001b[A\n",
            "Downloading:  32% 555M/1.72G [00:06<00:12, 92.5MB/s]\u001b[A\n",
            "Downloading:  33% 564M/1.72G [00:06<00:13, 86.1MB/s]\u001b[A\n",
            "Downloading:  33% 574M/1.72G [00:06<00:12, 89.9MB/s]\u001b[A\n",
            "Downloading:  34% 583M/1.72G [00:06<00:12, 90.0MB/s]\u001b[A\n",
            "Downloading:  34% 592M/1.72G [00:06<00:12, 87.3MB/s]\u001b[A\n",
            "Downloading:  35% 601M/1.72G [00:06<00:12, 88.9MB/s]\u001b[A\n",
            "Downloading:  36% 610M/1.72G [00:06<00:12, 88.1MB/s]\u001b[A\n",
            "Downloading:  36% 620M/1.72G [00:07<00:12, 91.4MB/s]\u001b[A\n",
            "Downloading:  37% 630M/1.72G [00:07<00:12, 84.2MB/s]\u001b[A\n",
            "Downloading:  37% 638M/1.72G [00:07<00:12, 84.8MB/s]\u001b[A\n",
            "Downloading:  38% 648M/1.72G [00:07<00:12, 88.7MB/s]\u001b[A\n",
            "Downloading:  38% 657M/1.72G [00:07<00:12, 88.1MB/s]\u001b[A\n",
            "Downloading:  39% 666M/1.72G [00:07<00:11, 90.1MB/s]\u001b[A\n",
            "Downloading:  39% 676M/1.72G [00:07<00:11, 90.9MB/s]\u001b[A\n",
            "Downloading:  40% 686M/1.72G [00:07<00:11, 93.1MB/s]\u001b[A\n",
            "Downloading:  40% 695M/1.72G [00:07<00:10, 94.8MB/s]\u001b[A\n",
            "Downloading:  41% 705M/1.72G [00:08<00:10, 94.8MB/s]\u001b[A\n",
            "Downloading:  42% 715M/1.72G [00:08<00:10, 96.3MB/s]\u001b[A\n",
            "Downloading:  42% 725M/1.72G [00:08<00:10, 97.0MB/s]\u001b[A\n",
            "Downloading:  43% 735M/1.72G [00:08<00:10, 94.6MB/s]\u001b[A\n",
            "Downloading:  43% 744M/1.72G [00:08<00:11, 87.6MB/s]\u001b[A\n",
            "Downloading:  44% 753M/1.72G [00:08<00:11, 83.3MB/s]\u001b[A\n",
            "Downloading:  44% 762M/1.72G [00:08<00:11, 85.4MB/s]\u001b[A\n",
            "Downloading:  45% 771M/1.72G [00:08<00:11, 86.1MB/s]\u001b[A\n",
            "Downloading:  45% 780M/1.72G [00:08<00:10, 88.7MB/s]\u001b[A\n",
            "Downloading:  46% 789M/1.72G [00:08<00:10, 88.9MB/s]\u001b[A\n",
            "Downloading:  46% 799M/1.72G [00:09<00:10, 90.6MB/s]\u001b[A\n",
            "Downloading:  47% 808M/1.72G [00:09<00:09, 91.3MB/s]\u001b[A\n",
            "Downloading:  48% 817M/1.72G [00:09<00:10, 87.1MB/s]\u001b[A\n",
            "Downloading:  48% 827M/1.72G [00:09<00:09, 90.3MB/s]\u001b[A\n",
            "Downloading:  49% 837M/1.72G [00:09<00:09, 92.4MB/s]\u001b[A\n",
            "Downloading:  49% 846M/1.72G [00:09<00:09, 88.0MB/s]\u001b[A\n",
            "Downloading:  50% 855M/1.72G [00:09<00:09, 89.3MB/s]\u001b[A\n",
            "Downloading:  50% 865M/1.72G [00:09<00:09, 91.0MB/s]\u001b[A\n",
            "Downloading:  51% 875M/1.72G [00:09<00:09, 93.2MB/s]\u001b[A\n",
            "Downloading:  51% 884M/1.72G [00:10<00:09, 88.2MB/s]\u001b[A\n",
            "Downloading:  52% 893M/1.72G [00:10<00:09, 90.1MB/s]\u001b[A\n",
            "Downloading:  52% 903M/1.72G [00:10<00:09, 89.4MB/s]\u001b[A\n",
            "Downloading:  53% 912M/1.72G [00:10<00:09, 87.2MB/s]\u001b[A\n",
            "Downloading:  54% 920M/1.72G [00:10<00:09, 86.5MB/s]\u001b[A\n",
            "Downloading:  54% 929M/1.72G [00:10<00:09, 87.6MB/s]\u001b[A\n",
            "Downloading:  55% 938M/1.72G [00:10<00:08, 88.2MB/s]\u001b[A\n",
            "Downloading:  55% 947M/1.72G [00:10<00:09, 84.8MB/s]\u001b[A\n",
            "Downloading:  56% 957M/1.72G [00:10<00:08, 87.7MB/s]\u001b[A\n",
            "Downloading:  56% 966M/1.72G [00:10<00:08, 88.7MB/s]\u001b[A\n",
            "Downloading:  57% 975M/1.72G [00:11<00:08, 90.5MB/s]\u001b[A\n",
            "Downloading:  57% 984M/1.72G [00:11<00:08, 87.5MB/s]\u001b[A\n",
            "Downloading:  58% 994M/1.72G [00:11<00:07, 90.7MB/s]\u001b[A\n",
            "Downloading:  58% 1.00G/1.72G [00:11<00:07, 91.4MB/s]\u001b[A\n",
            "Downloading:  59% 1.01G/1.72G [00:11<00:07, 90.4MB/s]\u001b[A\n",
            "Downloading:  59% 1.02G/1.72G [00:11<00:08, 86.3MB/s]\u001b[A\n",
            "Downloading:  60% 1.03G/1.72G [00:11<00:07, 87.1MB/s]\u001b[A\n",
            "Downloading:  60% 1.04G/1.72G [00:11<00:07, 89.7MB/s]\u001b[A\n",
            "Downloading:  61% 1.05G/1.72G [00:11<00:08, 83.6MB/s]\u001b[A\n",
            "Downloading:  62% 1.06G/1.72G [00:12<00:29, 22.8MB/s]\u001b[A\n",
            "Downloading:  62% 1.07G/1.72G [00:13<00:21, 29.9MB/s]\u001b[A\n",
            "Downloading:  63% 1.08G/1.72G [00:13<00:17, 37.7MB/s]\u001b[A\n",
            "Downloading:  63% 1.08G/1.72G [00:13<00:15, 40.2MB/s]\u001b[A\n",
            "Downloading:  64% 1.09G/1.72G [00:13<00:13, 47.4MB/s]\u001b[A\n",
            "Downloading:  64% 1.10G/1.72G [00:13<00:11, 55.2MB/s]\u001b[A\n",
            "Downloading:  65% 1.11G/1.72G [00:13<00:09, 61.7MB/s]\u001b[A\n",
            "Downloading:  65% 1.12G/1.72G [00:13<00:08, 67.7MB/s]\u001b[A\n",
            "Downloading:  66% 1.13G/1.72G [00:13<00:08, 73.7MB/s]\u001b[A\n",
            "Downloading:  66% 1.14G/1.72G [00:13<00:07, 78.3MB/s]\u001b[A\n",
            "Downloading:  67% 1.15G/1.72G [00:14<00:06, 82.0MB/s]\u001b[A\n",
            "Downloading:  67% 1.16G/1.72G [00:14<00:06, 84.0MB/s]\u001b[A\n",
            "Downloading:  68% 1.16G/1.72G [00:14<00:06, 87.2MB/s]\u001b[A\n",
            "Downloading:  68% 1.17G/1.72G [00:14<00:06, 90.5MB/s]\u001b[A\n",
            "Downloading:  69% 1.18G/1.72G [00:14<00:05, 91.4MB/s]\u001b[A\n",
            "Downloading:  69% 1.19G/1.72G [00:14<00:06, 80.5MB/s]\u001b[A\n",
            "Downloading:  70% 1.20G/1.72G [00:14<00:06, 82.4MB/s]\u001b[A\n",
            "Downloading:  70% 1.21G/1.72G [00:14<00:05, 86.7MB/s]\u001b[A\n",
            "Downloading:  71% 1.22G/1.72G [00:14<00:06, 82.2MB/s]\u001b[A\n",
            "Downloading:  71% 1.23G/1.72G [00:15<00:06, 78.8MB/s]\u001b[A\n",
            "Fetching 15 files:  80% 12/15 [00:25<00:02,  1.48it/s]\n",
            "Downloading:  73% 1.25G/1.72G [00:15<00:05, 86.5MB/s]\u001b[A\n",
            "Downloading:  73% 1.26G/1.72G [00:15<00:05, 83.3MB/s]\u001b[A\n",
            "Downloading:  74% 1.27G/1.72G [00:15<00:05, 86.8MB/s]\u001b[A\n",
            "Downloading:  74% 1.28G/1.72G [00:15<00:04, 88.7MB/s]\u001b[A\n",
            "Downloading:  75% 1.28G/1.72G [00:15<00:04, 89.3MB/s]\u001b[A\n",
            "Downloading:  75% 1.29G/1.72G [00:15<00:05, 81.5MB/s]\u001b[A\n",
            "Downloading:  76% 1.30G/1.72G [00:15<00:04, 85.5MB/s]\u001b[A\n",
            "Downloading:  76% 1.31G/1.72G [00:15<00:04, 87.3MB/s]\u001b[A\n",
            "Downloading:  77% 1.32G/1.72G [00:16<00:04, 82.9MB/s]\u001b[A\n",
            "Downloading:  77% 1.33G/1.72G [00:16<00:04, 85.8MB/s]\u001b[A\n",
            "Downloading:  78% 1.34G/1.72G [00:16<00:04, 88.7MB/s]\u001b[A\n",
            "Downloading:  79% 1.35G/1.72G [00:16<00:04, 90.3MB/s]\u001b[A\n",
            "Downloading:  79% 1.36G/1.72G [00:16<00:04, 85.2MB/s]\u001b[A\n",
            "Downloading:  80% 1.37G/1.72G [00:16<00:03, 87.8MB/s]\u001b[A\n",
            "Downloading:  80% 1.38G/1.72G [00:16<00:04, 84.0MB/s]\u001b[A\n",
            "Downloading:  81% 1.39G/1.72G [00:16<00:03, 84.3MB/s]\u001b[A\n",
            "Downloading:  81% 1.39G/1.72G [00:16<00:04, 80.9MB/s]\u001b[A\n",
            "Downloading:  82% 1.40G/1.72G [00:17<00:03, 83.7MB/s]\u001b[A\n",
            "Downloading:  82% 1.41G/1.72G [00:17<00:03, 85.3MB/s]\u001b[A\n",
            "Downloading:  83% 1.42G/1.72G [00:17<00:03, 83.7MB/s]\u001b[A\n",
            "Downloading:  83% 1.43G/1.72G [00:17<00:03, 85.6MB/s]\u001b[A\n",
            "Downloading:  84% 1.44G/1.72G [00:17<00:03, 88.0MB/s]\u001b[A\n",
            "Downloading:  84% 1.45G/1.72G [00:17<00:03, 89.9MB/s]\u001b[A\n",
            "Downloading:  85% 1.46G/1.72G [00:17<00:03, 82.8MB/s]\u001b[A\n",
            "Downloading:  85% 1.47G/1.72G [00:17<00:03, 82.7MB/s]\u001b[A\n",
            "Downloading:  86% 1.48G/1.72G [00:17<00:02, 87.0MB/s]\u001b[A\n",
            "Downloading:  86% 1.49G/1.72G [00:17<00:02, 88.3MB/s]\u001b[A\n",
            "Downloading:  87% 1.49G/1.72G [00:18<00:02, 84.2MB/s]\u001b[A\n",
            "Downloading:  87% 1.50G/1.72G [00:18<00:02, 88.0MB/s]\u001b[A\n",
            "Downloading:  88% 1.51G/1.72G [00:18<00:02, 86.3MB/s]\u001b[A\n",
            "Downloading:  88% 1.52G/1.72G [00:18<00:02, 79.7MB/s]\u001b[A\n",
            "Downloading:  89% 1.53G/1.72G [00:18<00:02, 83.1MB/s]\u001b[A\n",
            "Downloading:  90% 1.54G/1.72G [00:18<00:02, 85.5MB/s]\u001b[A\n",
            "Downloading:  90% 1.55G/1.72G [00:18<00:02, 83.7MB/s]\u001b[A\n",
            "Downloading:  91% 1.56G/1.72G [00:18<00:01, 84.4MB/s]\u001b[A\n",
            "Downloading:  91% 1.57G/1.72G [00:18<00:01, 81.6MB/s]\u001b[A\n",
            "Downloading:  92% 1.57G/1.72G [00:19<00:01, 82.7MB/s]\u001b[A\n",
            "Downloading:  92% 1.58G/1.72G [00:19<00:01, 86.3MB/s]\u001b[A\n",
            "Downloading:  93% 1.59G/1.72G [00:19<00:01, 87.9MB/s]\u001b[A\n",
            "Downloading:  93% 1.60G/1.72G [00:19<00:01, 90.8MB/s]\u001b[A\n",
            "Downloading:  94% 1.61G/1.72G [00:19<00:01, 80.9MB/s]\u001b[A\n",
            "Downloading:  94% 1.62G/1.72G [00:19<00:01, 85.0MB/s]\u001b[A\n",
            "Downloading:  95% 1.63G/1.72G [00:19<00:01, 87.7MB/s]\u001b[A\n",
            "Downloading:  95% 1.64G/1.72G [00:19<00:00, 85.2MB/s]\u001b[A\n",
            "Downloading:  96% 1.65G/1.72G [00:19<00:00, 89.2MB/s]\u001b[A\n",
            "Downloading:  96% 1.66G/1.72G [00:19<00:00, 90.8MB/s]\u001b[A\n",
            "Downloading:  97% 1.67G/1.72G [00:20<00:00, 91.4MB/s]\u001b[A\n",
            "Downloading:  98% 1.68G/1.72G [00:20<00:00, 93.6MB/s]\u001b[A\n",
            "Downloading:  98% 1.69G/1.72G [00:20<00:00, 94.0MB/s]\u001b[A\n",
            "Downloading:  99% 1.70G/1.72G [00:20<00:00, 94.3MB/s]\u001b[A\n",
            "Downloading:  99% 1.71G/1.72G [00:20<00:00, 93.6MB/s]\u001b[A\n",
            "Downloading: 100% 1.72G/1.72G [00:20<00:00, 83.0MB/s]\n",
            "Fetching 15 files:  87% 13/15 [00:31<00:08,  4.47s/it]\n",
            "Downloading: 100% 609/609 [00:00<00:00, 617kB/s]\n",
            "\n",
            "Downloading:   0% 0.00/167M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading:   5% 8.93M/167M [00:00<00:01, 89.3MB/s]\u001b[A\n",
            "Downloading:  11% 17.9M/167M [00:00<00:01, 88.4MB/s]\u001b[A\n",
            "Downloading:  16% 26.7M/167M [00:00<00:01, 85.8MB/s]\u001b[A\n",
            "Downloading:  21% 35.3M/167M [00:00<00:01, 86.0MB/s]\u001b[A\n",
            "Downloading:  26% 44.2M/167M [00:00<00:01, 87.0MB/s]\u001b[A\n",
            "Downloading:  32% 53.2M/167M [00:00<00:01, 87.9MB/s]\u001b[A\n",
            "Downloading:  38% 62.9M/167M [00:00<00:01, 90.9MB/s]\u001b[A\n",
            "Downloading:  43% 72.0M/167M [00:00<00:01, 90.2MB/s]\u001b[A\n",
            "Downloading:  49% 81.2M/167M [00:00<00:00, 90.9MB/s]\u001b[A\n",
            "Downloading:  54% 90.3M/167M [00:01<00:00, 83.7MB/s]\u001b[A\n",
            "Downloading:  59% 99.4M/167M [00:01<00:00, 85.7MB/s]\u001b[A\n",
            "Downloading:  65% 108M/167M [00:01<00:00, 76.2MB/s] \u001b[A\n",
            "Downloading:  70% 117M/167M [00:01<00:00, 78.8MB/s]\u001b[A\n",
            "Downloading:  75% 126M/167M [00:01<00:00, 82.2MB/s]\u001b[A\n",
            "Downloading:  80% 135M/167M [00:01<00:00, 84.1MB/s]\u001b[A\n",
            "Downloading:  86% 143M/167M [00:01<00:00, 79.9MB/s]\u001b[A\n",
            "Downloading:  91% 152M/167M [00:01<00:00, 82.4MB/s]\u001b[A\n",
            "Downloading: 100% 167M/167M [00:01<00:00, 84.7MB/s]\n",
            "Fetching 15 files: 100% 15/15 [00:33<00:00,  2.22s/it]\n",
            "Generating class images: 100% 50/50 [26:50<00:00, 32.21s/it]\n",
            "Downloading: 100% 1.06M/1.06M [00:00<00:00, 70.1MB/s]\n",
            "Downloading: 100% 525k/525k [00:00<00:00, 21.3MB/s]\n",
            "Downloading: 100% 472/472 [00:00<00:00, 454kB/s]\n",
            "Downloading: 100% 822/822 [00:00<00:00, 935kB/s]\n",
            "Downloading: 100% 636/636 [00:00<00:00, 567kB/s]\n",
            "Downloading: 100% 246M/246M [00:07<00:00, 31.0MB/s]\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
            "================================================================================\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:105: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-lsmjagv2u5xx --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--listen_host=172.28.0.2 --target_host=172.28.0.2 --tunnel_background_save_url=https')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('6000,\"kernelManagerProxyHost\"'), PosixPath('{\"kernelManagerProxyPort\"'), PosixPath('true}'), PosixPath('[\"--ip=172.28.0.2\"],\"debugAdapterMultiplexerPath\"'), PosixPath('\"172.28.0.2\",\"jupyterArgs\"'), PosixPath('\"/usr/local/bin/dap_multiplexer\",\"enableLsp\"')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/paths.py:27: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 112\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda112.so...\n",
            "/usr/local/lib/python3.8/dist-packages/diffusers/utils/deprecation_utils.py:35: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
            "  warnings.warn(warning + message, FutureWarning)\n",
            "Downloading: 100% 308/308 [00:00<00:00, 290kB/s]\n",
            "Caching latents: 100% 200/200 [00:42<00:00,  4.68it/s]\n",
            "***** Running training *****\n",
            "  Num examples = 200\n",
            "  Num batches each epoch = 200\n",
            "  Num Epochs = 40\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 8000\n",
            "Steps:  25% 2000/8000 [29:14<1:27:12,  1.15it/s, loss=0.276, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 12129.28it/s]\n",
            "/usr/local/lib/python3.8/dist-packages/diffusers/utils/deprecation_utils.py:35: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
            "  \"_class_name\": \"DDIMScheduler\",\n",
            "  \"_diffusers_version\": \"0.9.0\",\n",
            "  \"beta_end\": 0.012,\n",
            "  \"beta_schedule\": \"scaled_linear\",\n",
            "  \"beta_start\": 0.00085,\n",
            "  \"clip_sample\": false,\n",
            "  \"num_train_timesteps\": 1000,\n",
            "  \"prediction_type\": \"epsilon\",\n",
            "  \"set_alpha_to_one\": false,\n",
            "  \"steps_offset\": 0,\n",
            "  \"trained_betas\": null\n",
            "}\n",
            " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
            "  warnings.warn(warning + message, FutureWarning)\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[Alogging sample image to wandb, step: 2000\n",
            "\n",
            "Generating samples:  25% 1/4 [00:12<00:37, 12.52s/it]\u001b[Alogging sample image to wandb, step: 2000\n",
            "\n",
            "Generating samples:  50% 2/4 [00:21<00:20, 10.39s/it]\u001b[Alogging sample image to wandb, step: 2000\n",
            "\n",
            "Generating samples:  75% 3/4 [00:30<00:09,  9.79s/it]\u001b[Alogging sample image to wandb, step: 2000\n",
            "\n",
            "Generating samples: 100% 4/4 [00:39<00:00,  9.90s/it]\n",
            "[*] Weights saved at /content/drive/MyDrive/stable_diffusion_weights/live_subjects/clcyv4i9d000108kxbn41b79k/2000\n",
            "Steps:  33% 2640/8000 [39:33<1:17:59,  1.15it/s, loss=0.278, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 9564.39it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[Alogging sample image to wandb, step: 2640\n",
            "\n",
            "Generating samples:  25% 1/4 [00:08<00:26,  8.82s/it]\u001b[Alogging sample image to wandb, step: 2640\n",
            "\n",
            "Generating samples:  50% 2/4 [00:17<00:17,  8.89s/it]\u001b[Alogging sample image to wandb, step: 2640\n",
            "\n",
            "Generating samples:  75% 3/4 [00:26<00:08,  8.99s/it]\u001b[Alogging sample image to wandb, step: 2640\n",
            "\n",
            "Generating samples: 100% 4/4 [00:35<00:00,  8.98s/it]\n",
            "[*] Weights saved at /content/drive/MyDrive/stable_diffusion_weights/live_subjects/clcyv4i9d000108kxbn41b79k/2640\n",
            "Steps:  38% 3000/8000 [45:44<1:12:35,  1.15it/s, loss=0.275, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 13919.15it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[Alogging sample image to wandb, step: 3000\n",
            "\n",
            "Generating samples:  25% 1/4 [00:08<00:26,  8.68s/it]\u001b[Alogging sample image to wandb, step: 3000\n",
            "\n",
            "Generating samples:  50% 2/4 [00:17<00:17,  8.77s/it]\u001b[Alogging sample image to wandb, step: 3000\n",
            "\n",
            "Generating samples:  75% 3/4 [00:26<00:08,  8.88s/it]\u001b[Alogging sample image to wandb, step: 3000\n",
            "\n",
            "Generating samples: 100% 4/4 [00:35<00:00,  8.93s/it]\n",
            "[*] Weights saved at /content/drive/MyDrive/stable_diffusion_weights/live_subjects/clcyv4i9d000108kxbn41b79k/3000\n",
            "Steps:  50% 4000/8000 [1:01:12<58:12,  1.15it/s, loss=0.273, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 10477.03it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[Alogging sample image to wandb, step: 4000\n",
            "\n",
            "Generating samples:  25% 1/4 [00:08<00:26,  8.96s/it]\u001b[Alogging sample image to wandb, step: 4000\n",
            "\n",
            "Generating samples:  50% 2/4 [00:17<00:17,  8.98s/it]\u001b[Alogging sample image to wandb, step: 4000\n",
            "\n",
            "Generating samples:  75% 3/4 [00:27<00:09,  9.05s/it]\u001b[Alogging sample image to wandb, step: 4000\n",
            "\n",
            "Generating samples: 100% 4/4 [00:36<00:00,  9.03s/it]\n",
            "[*] Weights saved at /content/drive/MyDrive/stable_diffusion_weights/live_subjects/clcyv4i9d000108kxbn41b79k/4000\n",
            "Steps:  62% 5000/8000 [1:16:41<43:36,  1.15it/s, loss=0.271, lr=1e-6]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 9718.04it/s]\n",
            "\n",
            "Generating samples:   0% 0/4 [00:00<?, ?it/s]\u001b[Alogging sample image to wandb, step: 5000\n",
            "\n",
            "Generating samples:  25% 1/4 [00:08<00:26,  8.75s/it]\u001b[Alogging sample image to wandb, step: 5000\n",
            "\n",
            "Generating samples:  50% 2/4 [00:17<00:17,  8.83s/it]\u001b[Alogging sample image to wandb, step: 5000\n",
            "\n",
            "Generating samples:  75% 3/4 [00:26<00:08,  8.96s/it]\u001b[Alogging sample image to wandb, step: 5000\n",
            "\n",
            "Generating samples: 100% 4/4 [00:35<00:00,  8.99s/it]\n",
            "[*] Weights saved at /content/drive/MyDrive/stable_diffusion_weights/live_subjects/clcyv4i9d000108kxbn41b79k/5000\n",
            "Steps:  64% 5087/8000 [1:18:54<42:21,  1.15it/s, loss=0.27, lr=1e-6]Traceback (most recent call last):\n",
            "  File \"train_dreambooth.py\", line 1057, in <module>\n",
            "    main(args)\n",
            "  File \"train_dreambooth.py\", line 972, in main\n",
            "    optimizer.step()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/accelerate/optimizer.py\", line 136, in step\n",
            "    scale_after = self.scaler.get_scale()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py\", line 417, in get_scale\n",
            "    return self._init_scale if self._scale is None else self._get_scale_async().item()\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/accelerate/commands/launch.py\", line 1104, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/accelerate/commands/launch.py\", line 565, in simple_launcher\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.8/subprocess.py\", line 1083, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.8/subprocess.py\", line 1806, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.8/subprocess.py\", line 1764, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss 0.09007\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33molive-puddle-11\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rdp/dreambooth-live-subjects/runs/bky1861i\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 6 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230117_100724-bky1861i/logs\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch train_dreambooth.py \\\n",
        "  --save_special_weights=2640 \\\n",
        "  --wandb_project=\"dreambooth-live-subjects\" \\\n",
        "  --prior_preservation_images=\"generated\" \\\n",
        "  --minimum_save_step=1999 \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=200 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=8000 \\\n",
        "  --save_interval=1000 \\\n",
        "  --save_sample_prompt=f\"photo of {token} {gender}\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89Az5NUxOWdy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "29eaf3b0-5fdb-45ab-c8cd-72d409da6735"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1046691759c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mWEIGHTS_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "22#@markdown Specify the weights directory to use (leave blank for latest)\n",
        "WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACUL_oKlLwBt"
      },
      "outputs": [],
      "source": [
        "#@markdown Run to generate a grid of preview images from the last saved weights.\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "weights_folder = OUTPUT_DIR\n",
        "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
        "\n",
        "row = len(folders)\n",
        "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
        "scale = 4\n",
        "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "    folder_path = os.path.join(weights_folder, folder)\n",
        "    image_folder = os.path.join(folder_path, \"samples\")\n",
        "    images = [f for f in os.listdir(image_folder)]\n",
        "    for j, image in enumerate(images):\n",
        "        if row == 1:\n",
        "            currAxes = axes[j]\n",
        "        else:\n",
        "            currAxes = axes[i, j]\n",
        "        if i == 0:\n",
        "            currAxes.set_title(f\"Image {j}\")\n",
        "        if j == 0:\n",
        "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        img = mpimg.imread(image_path)\n",
        "        currAxes.imshow(img, cmap='gray')\n",
        "        currAxes.axis('off')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.savefig('grid.png', dpi=72)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V8wgU0HN-Kq"
      },
      "source": [
        "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXzsUyG1aCy"
      },
      "outputs": [],
      "source": [
        "#@markdown Run conversion.\n",
        "ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
        "\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNG4fd_dTbF"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW15FjffdTID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "\n",
        "g_cuda = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIzkltjpVO_f"
      },
      "outputs": [],
      "source": [
        "#@markdown Can set random seed here for reproducibility.\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6xoHWSsbcS3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#@title Run for generating images.\n",
        "\n",
        "prompt = \"photo of spoon\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "num_samples = 4 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "num_inference_steps = 50 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WMCqQ5Tcdsm2"
      },
      "outputs": [],
      "source": [
        "#@markdown Run Gradio UI for generating images.\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        return pipe(\n",
        "                prompt, height=int(height), width=int(width),\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_images_per_prompt=int(num_samples),\n",
        "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "                generator=g_cuda\n",
        "            ).images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=50)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lJoOgLQHnC8L"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Delete diffuser and old weights and only keep the ckpt to free up drive space.\n",
        "\n",
        "#@markdown [ ! ] Caution, Only execute if you are sure u want to delete the diffuser format weights and only use the ckpt.\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "for f in glob(OUTPUT_DIR+os.sep+\"*\"):\n",
        "    if f != WEIGHTS_DIR:\n",
        "        shutil.rmtree(f)\n",
        "        print(\"Deleted\", f)\n",
        "for f in glob(WEIGHTS_DIR+\"/*\"):\n",
        "    if not f.endswith(\".ckpt\") or not f.endswith(\".json\"):\n",
        "        try:\n",
        "            shutil.rmtree(f)\n",
        "        except NotADirectoryError:\n",
        "            continue\n",
        "        print(\"Deleted\", f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXgi8HM4c-DA"
      },
      "outputs": [],
      "source": [
        "#@title Free runtime memory\n",
        "exit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDavPmgUp5Fn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}